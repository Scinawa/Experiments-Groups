{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate experiments with new skew-spectrums"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( This is done using my sage310 environment )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.io\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#import tensorflow as tf\n",
    "import time\n",
    "from sklearn.decomposition  import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from spectrum_utils import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "augmentation_factor = 0  # how much did we augment the dataset? (this is our NeurIPS trick)\n",
    "num_atoms = 23\n",
    "verbose=True\n",
    "y_scaling_factor = 2000.\n",
    "\n",
    "rand_state = 42\n",
    "#tf.random.set_seed(rand_state)\n",
    "np.random.seed(rand_state)\n",
    "\n",
    "\n",
    "# hyperparameters for classification\n",
    "total_range = 1787.119995\n",
    "num_bins = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "qm7 = scipy.io.loadmat('/Users/scinawa/workspace/grouptheoretical/hpctmp/t0926977/multi_orbit_bispectrum/qm7.mat')\n",
    "R = qm7['R']\n",
    "\n",
    "y = np.transpose(qm7['T']).reshape((7165,))\n",
    "y_scaling_factor = 2000.\n",
    "y_scaled = y / y_scaling_factor\n",
    "\n",
    "\n",
    "\n",
    "directory='/hpctmp/t0926977/multi_orbit_bispectrum/qm7-skew_spectrums/'\n",
    "\n",
    "# file_1full = '1orbit_qm7_full_skew_spectrum.npy'\n",
    "# file_2full = '2orbit_qm7_full_skew_spectrum.npy'\n",
    "\n",
    "# file_1redu = '1orbit_qm7_reduced_skew_spectrum.npy'\n",
    "# file_2redu = '2orbit_qm7_reduced_skew_spectrum.npy'\n",
    "\n",
    "# file_1redu3 = \"1orbit_qm7_reduced_3skew_spectrum.npy\"\n",
    "# file_2redu3 = \"2orbit_qm7_reduced_3skew_spectrum.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_dist(x):\n",
    "    x[x == 0] = np.nan\n",
    "    return np.nanmean(x, axis=0)\n",
    "\n",
    "\n",
    "def extract_dataset():\n",
    "    iu = np.triu_indices(num_atoms,k=0) \n",
    "    iu_dist = np.triu_indices(num_atoms,k=1) # for the pairwise distance matrix, all diagonol entries will be 0 \n",
    "    CM = np.zeros((qm7['X'].shape[0], num_atoms*(num_atoms+1)//2), dtype=float)\n",
    "    eigs = np.zeros((qm7['X'].shape[0], num_atoms), dtype=float)\n",
    "    centralities = np.zeros((qm7['X'].shape[0], num_atoms), dtype=float)\n",
    "    interatomic_dist = np.zeros((qm7['X'].shape[0], ((num_atoms*num_atoms)-num_atoms)//2), dtype=float) \n",
    "\n",
    "\n",
    "    for i, cm in enumerate(qm7['X']):\n",
    "        coulomb_vector = cm[iu]\n",
    "        # Sort elements by decreasing order\n",
    "        shuffle = np.argsort(-coulomb_vector)\n",
    "        CM[i] = coulomb_vector[shuffle]\n",
    "        dist = squareform(pdist(R[i]))\n",
    "        # we can extract the upper triangle of the distance matri: return vector of dimension (1,num_atoms)\n",
    "        dist_vector = dist[iu_dist]\n",
    "        shuffle = np.argsort(-dist_vector)\n",
    "        interatomic_dist[i] = dist_vector[shuffle]\n",
    "        \n",
    "        w,v = np.linalg.eig((dist))\n",
    "        eigs[i] = w[np.argsort(-w)]\n",
    "        centralities[i] = np.array(list(nx.eigenvector_centrality(nx.Graph(dist)).values()))\n",
    "        \n",
    "        if verbose and i % 500 == 0:\n",
    "            print(\"Processed {} molecules\".format(i))\n",
    "        \n",
    "    mean_dists = np.apply_along_axis(mean_dist, axis=1, arr=interatomic_dist)\n",
    "\n",
    "    return qm7['X'], CM, eigs, centralities, interatomic_dist, mean_dists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dictionary of feature that will be used for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 molecules\n",
      "Processed 500 molecules\n",
      "Processed 1000 molecules\n",
      "Processed 1500 molecules\n",
      "Processed 2000 molecules\n",
      "Processed 2500 molecules\n",
      "Processed 3000 molecules\n",
      "Processed 3500 molecules\n",
      "Processed 4000 molecules\n",
      "Processed 4500 molecules\n",
      "Processed 5000 molecules\n",
      "Processed 5500 molecules\n",
      "Processed 6000 molecules\n",
      "Processed 6500 molecules\n",
      "Processed 7000 molecules\n"
     ]
    }
   ],
   "source": [
    "qm7['X'], CM, eigs, centralities, interatomic_dist, mean_dists = extract_dataset()\n",
    "megadataset= {'qm7': qm7['X'], 'CM' : CM, 'eigs': eigs, 'centralities': centralities, \n",
    "                   'interatomic_dist': interatomic_dist, 'mean_dist': mean_dists }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kreating 4-th correlation\n",
      "kreating 5-th correlation\n"
     ]
    }
   ],
   "source": [
    "for k in range(4,6):\n",
    "\tnew_kcor1orb = []\n",
    "\tnew_kcor2orb = []\n",
    "\tprint(\"kreating {}-th correlation\".format(k))\n",
    "\tfor i in range(len(qm7['X'])):\n",
    "\n",
    "\t\tfunc_1o = create_func_on_group_from_matrix_1orbit(qm7['X'][i].reshape(23,23))\n",
    "\t\tfunc_2o = create_func_on_group_from_matrix_2orbits(qm7['X'][i].reshape(23,23))\n",
    "\n",
    "\n",
    "\t\tnew_kcor1orb.append(reduced_k_correlation(func_1o, k=k, method=\"extremedyn\", vector=True))     # vector = False !!!!!\n",
    "\t\tnew_kcor2orb.append(reduced_k_correlation(func_2o, k=k, method=\"extremedyn\", vector=True))\n",
    "\n",
    "\tmegadataset[\"1-{}-corre\".format(k)] = new_kcor1orb\n",
    "\tmegadataset[\"2-{}-corre\".format(k)] = new_kcor2orb\n",
    "\n",
    "############ USE WITH CARE!!!!!! IT WILL OVERWRITE THE PERVIOUSLY COMPUTED FEATURES FILE\n",
    "with open('megadump-all-features.pickle', 'wb') as handle:\n",
    "   pickle.dump(megadataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = open('megadump-all-features.pickle', 'rb')\n",
    "megadataset = pickle.load(handle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of machine learning models\n",
    "This code is for the machine learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "#from catboost import CatBoostRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "#from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "\n",
    "models = {'GradientBR' :  GradientBoostingRegressor, 'Elastic' : ElasticNet, \n",
    "          #'SGD' : SGDRegressor, \n",
    "          # 'SVR': SVR, \n",
    "          #'Bayesian': BayesianRidge,\n",
    "          #'CatBoost': CatBoostRegressor, \n",
    "          #'KernelRidge': KernelRidge, \n",
    "          'Linear': LinearRegression, 'XGBR': XGBRegressor } #'LGMR': LGBMRegressor}\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import sys\n",
    "import pickle\n",
    "warnings.simplefilter('ignore')\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def machine_learning_models(X_train, y_train, X_test, y_test):\n",
    "\n",
    "\n",
    "    for model_name,model in models.items():\n",
    "        print(\"Working with \", model_name, \". Error: \", end=\"\")\n",
    "        #if model_name == \"SGD\":\n",
    "        #    import pdb\n",
    "        #    pdb.set_trace()\n",
    "        #    clf = model()\n",
    "        clf = model()\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print(\"(test)\", mean_absolute_error(y_pred , y_test)) \n",
    "\n",
    "\n",
    "\n",
    "def machine_learning_xgboost(X_train, y_train, X_test, y_test, X_val, y_val):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # n_folds = 5\n",
    "    early_stopping = 50\n",
    "\n",
    "    # xg_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    num_iters = 300\n",
    "    params = {\"objective\":\"reg:squarederror\",    # {\"objective\":\"reg:linear\", \n",
    "            'booster': 'gbtree', \n",
    "            'eval_metric': 'mae',\n",
    "            'subsample': 0.9,\n",
    "            'colsample_bytree':0.2,\n",
    "            'learning_rate': 0.05,\n",
    "            'max_depth': 6, \n",
    "            'reg_lambda': .9, \n",
    "            'reg_alpha': .01,\n",
    "            'seed': rand_state}\n",
    "\n",
    "    # cv = xgb.cv(params,\n",
    "    #             xg_train, \n",
    "    #             num_boost_round=num_iters, \n",
    "    #             nfold=n_folds, \n",
    "    #             early_stopping_rounds=early_stopping, \n",
    "    #             verbose_eval = 0, \n",
    "    #             seed=rand_state,\n",
    "    #             as_pandas=False)\n",
    "\n",
    "\n",
    "    # plt.figure(figsize=(8,8))\n",
    "    # plt.plot(cv['train-mae-mean'][100:], label='Train loss: ' + str(np.min(cv['train-mae-mean'])))\n",
    "    # plt.plot(cv['test-mae-mean'][100:], label='Test loss: ' + str(np.min(cv['test-mae-mean'])))\n",
    "    # plt.legend()\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Mean absolute error')\n",
    "    # plt.savefig(\"verybad.png\")\n",
    "\n",
    "    model_xgb = xgb.XGBRegressor(**params, random_state=rand_state, n_estimators=num_iters)\n",
    "    model_xgb.fit(X_train, y_train, \n",
    "                early_stopping_rounds=early_stopping, \n",
    "                #eval_metric='mae', \n",
    "                eval_set=[(X_val, y_val)], \n",
    "                verbose=False)\n",
    "\n",
    "\n",
    "    y_train_pred = model_xgb.predict(X_train)\n",
    "    print('Train mean absoulte error: ', mean_absolute_error(y_train, y_train_pred))\n",
    "\n",
    "    y_val_pred = model_xgb.predict(X_val)\n",
    "    print('Validation mean absoulte error: ', mean_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "\n",
    "    y_test_pred = model_xgb.predict(X_test)\n",
    "    print('TEST mean absoulte error: ', mean_absolute_error(y_test, y_test_pred))\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning workbench\n",
    "\n",
    "In the code below we can play with experiments, by changing the feature in the dataset and running the experiments that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['qm7', 'CM', 'eigs', 'centralities', 'interatomic_dist', 'mean_dist', '1-2-corre', '2-2-corre', '1-3-corre', '2-3-corre', '1-4-corre', '2-4-corre', '1-5-corre', '2-5-corre'])\n"
     ]
    }
   ],
   "source": [
    "def PCA_everything(dataset, ncomponents):\n",
    "    pca = PCA(n_components= ncomponents)\n",
    "    return pca.fit_transform(dataset)\n",
    "print(megadataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this experiemnt we do machine learning only with the k-correlations for all the ks, and all the orbits.\n",
      "1-2-corre\n",
      "Working with:  GradientBR Error: TEST 10.941690427068586\n",
      "Working with:  Elastic Error: TEST 28.242899458574186\n",
      "Working with:  Linear Error: TEST 9620.899963108961\n",
      "Working with:  XGBR Error: TEST 8.267371\n",
      "1-3-corre\n",
      "Working with:  GradientBR Error: TEST 10.868614833002184\n",
      "Working with:  Elastic Error: TEST 28.660986459607603\n",
      "Working with:  Linear Error: TEST 21.342878473041843\n",
      "Working with:  XGBR Error: TEST 8.470072\n",
      "1-4-corre\n",
      "Working with:  GradientBR Error: TEST 10.82198118300598\n",
      "Working with:  Elastic Error: TEST 27.883955706340625\n",
      "Working with:  Linear Error: TEST 21.834101490299084\n",
      "Working with:  XGBR Error: TEST 8.030998\n",
      "2-2-corre\n",
      "Working with:  GradientBR Error: TEST 10.64711116646851\n",
      "Working with:  Elastic Error: TEST 28.097465672239032\n",
      "Working with:  Linear Error: TEST 19.192725609970584\n",
      "Working with:  XGBR Error: TEST 8.209346\n",
      "2-3-corre\n",
      "Working with:  GradientBR Error: TEST 10.780972605006072\n",
      "Working with:  Elastic Error: TEST 26.82731349074745\n",
      "Working with:  Linear Error: TEST 19.790298760468108\n",
      "Working with:  XGBR Error: TEST 8.110941\n",
      "2-4-corre\n",
      "Working with:  GradientBR Error: TEST 10.710683842582158\n",
      "Working with:  Elastic Error: TEST 26.025591227775003\n",
      "Working with:  Linear Error: TEST 20.256685830066328\n",
      "Working with:  XGBR Error: TEST 7.971604\n"
     ]
    }
   ],
   "source": [
    "print(\"In this experiemnt we do machine learning only with the k-correlations for all the ks, and all the orbits.\")\n",
    "for orbit, k in [(orbit, k) for orbit in range(1,3) for k in range(2, 6) ]:\n",
    "    print(\"{}-{}-corre\".format(orbit,k))\n",
    "    X = np.concatenate((megadataset['CM'], megadataset['eigs'], PCA_everything(megadataset[\"{}-{}-corre\".format(orbit,k)], ncomponents=20) ), axis=1)\n",
    "    \n",
    "    # with cross validation no need to further split the data. if not using cross validation you should do one...\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                test_size=0.20, \n",
    "                                                random_state=rand_state)\n",
    "\n",
    "    machine_learning_models(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare 2,3 correlations together, by putting only this in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this experiemnt we do machine learning only with the k-correlations for all the ks, and all the orbits.\n",
      "Working with  GradientBR . Error: (test) 36.89754080618257\n",
      "Working with  Elastic . Error: (test) 112.3235028317343\n",
      "Working with  Linear . Error: (test) 61.087242\n",
      "Working with  XGBR . Error: (test) 28.977999\n",
      "Working with  GradientBR . Error: (test) 42.26125282194708\n",
      "Working with  Elastic . Error: (test) 83.3554355955342\n",
      "Working with  Linear . Error: (test) 66.10057\n",
      "Working with  XGBR . Error: (test) 32.20713\n",
      "Working with  GradientBR . Error: "
     ]
    }
   ],
   "source": [
    "print(\"In this experiemnt we do machine learning only with the k-correlations for all the ks, and all the orbits.\")\n",
    "for orbit, k in [(orbit, k) for orbit in range(1,3) for k in range(2, 6) ]:\n",
    "    #print(\"%sorbit-%s-corre\")\n",
    "    print (\"orbit\", orbit, \"k\", k)\n",
    "    X = megadataset[\"{0}-{1}-corre\".format(orbit,k)]\n",
    "    # with cross validation no need to further split the data. if not using cross validation you should do one...\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                test_size=0.20, \n",
    "                                                random_state=rand_state)\n",
    "\n",
    "    machine_learning_models(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare 2,3 correlations together, by putting only this in the dataset. This time we also do some PCA to reduce the dimensionality of the things. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In this experiemnt we do machine learning only with the k-correlations for all the ks, and all the orbits.\")\n",
    "for orbit, k in [(orbit, k) for orbit in range(1,3) for k in range(2, 5) ]:\n",
    "    print(\"%sorbit-%s-corre\")\n",
    "    X = megadataset[\"%s-%s-corre\".format(orbit,k)]\n",
    "    # with cross validation no need to further split the data. if not using cross validation you should do one...\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                test_size=0.20, \n",
    "                                                random_state=rand_state)\n",
    "\n",
    "    machine_learning_models(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #X = skspecqm7 \n",
    "    #X = megadataset['CM']\n",
    "    X = megadataset[\"2-2-corre\"]\n",
    "\n",
    "    #X = np.concatenate((megadataset['CM'], megadataset['eigs'], skspecqm7), axis=1)\n",
    "\n",
    "    # 2 full\n",
    "    #Dev mean absoulte error:  6.7378244\n",
    "    #Validation mean absoulte error:  6.7369103\n",
    "\n",
    "    X = np.concatenate((megadataset['CM'], megadataset['eigs'], megadataset['2-3-corre']), axis=1)\n",
    "    #X = np.concatenate((megadataset['CM'], megadataset['eigs'], megadataset['2-2-corre']), axis=1)\n",
    "    # 1 full\n",
    "    # Dev mean absoulte error:  7.145836\n",
    "    # Validation mean absoulte error:  6.9577074\n",
    "\n",
    "    #X = np.concatenate((skspecqm7, megadataset['CM'], megadataset['eigs'], megadataset['centralities']), axis=1)\n",
    "    # no skew_spectrum\n",
    "    #Dev mean absoulte error:  6.9068303\n",
    "    #Validation mean absoulte error:  6.810617\n",
    "\n",
    "    #X = skspecqm7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # with cross validation no need to further split the data. if not using cross validation you should do one...\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                            test_size=0.20, \n",
    "                                            random_state=rand_state)\n",
    "\n",
    "    #X_train, X_val, y_train, y_val = train_test_split(X_2, y_2, \n",
    "    #                                               test_size=0.18, \n",
    "    #                                               random_state=rand_state)\n",
    "\n",
    "    # try: \n",
    "    #     non_zero_idx = np.any(X_train[:, 0:len(skspecqm7[1]) ], axis=0)\n",
    "    #     non_zero_idx = np.concatenate( ( non_zero_idx, np.array([1] *(X.shape[1]-skspecqm7.shape[1]) )    ))\n",
    "    #     X_test=X_test[:,non_zero_idx]\n",
    "    #     X_val=X_val[:,non_zero_idx]\n",
    "    #     X_train=X_train[:,non_zero_idx]\n",
    "    \n",
    "    # except Exception:\n",
    "    #     print(\"skipping taking only nonzero components of skewspectrum\")\n",
    "\n",
    "    #machine_learning_xgboost(X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "    machine_learning_models(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #X = skspecqm7 \n",
    "    #X = megadataset['CM']\n",
    "    X = megadataset[\"2-2-corre\"]\n",
    "\n",
    "    #X = np.concatenate((megadataset['CM'], megadataset['eigs'], skspecqm7), axis=1)\n",
    "\n",
    "    # 2 full\n",
    "    #Dev mean absoulte error:  6.7378244\n",
    "    #Validation mean absoulte error:  6.7369103\n",
    "\n",
    "    X = np.concatenate((megadataset['CM'], megadataset['eigs'], megadataset['2-3-corre']), axis=1)\n",
    "    #X = np.concatenate((megadataset['CM'], megadataset['eigs'], megadataset['2-2-corre']), axis=1)\n",
    "    # 1 full\n",
    "    # Dev mean absoulte error:  7.145836\n",
    "    # Validation mean absoulte error:  6.9577074\n",
    "\n",
    "    #X = np.concatenate((skspecqm7, megadataset['CM'], megadataset['eigs'], megadataset['centralities']), axis=1)\n",
    "    # no skew_spectrum\n",
    "    #Dev mean absoulte error:  6.9068303\n",
    "    #Validation mean absoulte error:  6.810617\n",
    "\n",
    "    #X = skspecqm7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # with cross validation no need to further split the data. if not using cross validation you should do one...\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                            test_size=0.20, \n",
    "                                            random_state=rand_state)\n",
    "\n",
    "    #X_train, X_val, y_train, y_val = train_test_split(X_2, y_2, \n",
    "    #                                               test_size=0.18, \n",
    "    #                                               random_state=rand_state)\n",
    "\n",
    "    # try: \n",
    "    #     non_zero_idx = np.any(X_train[:, 0:len(skspecqm7[1]) ], axis=0)\n",
    "    #     non_zero_idx = np.concatenate( ( non_zero_idx, np.array([1] *(X.shape[1]-skspecqm7.shape[1]) )    ))\n",
    "    #     X_test=X_test[:,non_zero_idx]\n",
    "    #     X_val=X_val[:,non_zero_idx]\n",
    "    #     X_train=X_train[:,non_zero_idx]\n",
    "    \n",
    "    # except Exception:\n",
    "    #     print(\"skipping taking only nonzero components of skewspectrum\")\n",
    "\n",
    "    #machine_learning_xgboost(X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "    machine_learning_models(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
